import json
import csv
import re
import nltk
words = set(nltk.corpus.words.words())
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def generate_data():
    stat_list = []
    symptoms_list = []
    time_list = []
    data_list = []
    with open("json_files/stat_thrds.json", "r", encoding="utf8") as f0:
        i = 0
        for line in f0:
            stat_dict = {}
            i += 1
            print(i)
            stat = json.loads(line)
            stat_dict['thread_id'] = stat['_id']
            stat_dict['resolution_status'] = stat['resolution_status']
            stat_dict['number_replies'] = stat['number_replies']
            stat_list.append(stat_dict)

    with open("json_files/symptoms_thrds.json", "r", encoding="utf8") as f1:
        i = 0
        for line in f1:
            symptoms_dict = {}
            i += 1
            print(i)
            symptoms = json.loads(line)
            symptoms_dict['thread_id'] = symptoms['_id']
            symptoms = symptoms['symptom']
            symptoms = ','.join(symptoms)
            symptoms_dict['symptoms'] = symptoms
            symptoms_list.append(symptoms_dict)


    with open("json_files/timeStat_thrds.json", "r", encoding="utf8") as f2:
        i = 0
        for line in f2:
            time_dict = {}
            i += 1
            print(i)
            time = json.loads(line)
            time_dict['thread_id'] = time['_id']
            startDate = time['startDate']
            startDate = startDate['$date']
            time_dict['startDate'] = startDate
            endDate = time['endDate']['$date']
            time_dict['endDate'] = endDate
            duration = time['duration']['$numberLong']
            time_dict['duration'] = duration
            time_list.append(time_dict)
        #print(stat_list)
    i = 0
    for stat in stat_list:
        for symptoms in symptoms_list:
            if stat['thread_id'] == symptoms['thread_id']:
                stat['symptoms'] = symptoms['symptoms']
        for time in time_list:
            if stat['thread_id'] == time['thread_id']:
                stat['startDate'] = time['startDate']
                stat['endDate'] = time['endDate']
                stat['duration'] = time['duration']
        data_list.append(stat)
        print(i)
        i += 1
    with open("data.csv", "a", encoding="utf8") as f3:
        keys = list(data_list[0].keys())
        writer = csv.DictWriter(f3, fieldnames=keys)
        writer.writeheader()
        writer.writerows(data_list)

#print(len(data_list))

# write to csv file
def read_data():
    with open("../data/data.csv", "r", encoding="utf8") as f4:
        data_list = []
        i = 0
        for line in f4:
            i += 1
            if(i > 1):
                data_item = line.split(',')
                #print(data_item)
                print(i)
                data_list.append(data_item)
    return data_list

def add_malware_data():
    with open("../../util/json_files/malware_attribution_thrds.json", "r", encoding="utf8") as f5:
        malware_list = []
        i = 0
        for line in f5:
            i += 1
            print(i)
            malware_dict = {}
            malware = json.loads(line)
            malware_dict['thread_id'] = malware['post_id']
            malware_labels = malware['label']
            malware_labels = ';'.join(malware_labels)
            malware_dict['malware'] = malware_labels
            malware_list.append(malware_dict)
        print(len(malware_list))

    with open("../../util/json_files/malware.csv", "a", encoding="utf8") as f6:
            keys = list(malware_list[0].keys())
            writer = csv.DictWriter(f6, fieldnames=keys)
            writer.writeheader()
            writer.writerows(malware_list)

def load_body():
    with open("../../util/json_files/thrds.json", "r", encoding="utf8") as f7:
        body_list = []
        i = 0
        for line in f7:

            i += 1
            print(i)
            body_dict = {}
            body = json.loads(line)
            if body["_id"]["comment_id"] == "1":
                body_dict["thread_id"] = body["_id"]["post_id"]
                body = body["body"]
                pattern_scan_frst = re.compile(r'(Scan result of Farbar Recovery Scan Tool \(FRST\))(.|\n)*(End of FRST.txt)')
                pattern_addition_frst = pattern = re.compile(r'(Additional scan result of Farbar Recovery Scan Tool)(.|\n)*(End of Addition.txt)')
                pattern_fix_frst = re.compile(r'(Fix result of Farbar Recovery Scan Tool )(.|\n)*(End of Fixlog)')
                pattern_hijackthis = re.compile(r'(Logfile of HijackThis)(.|\n)*')
                log1 = re.search(pattern_scan_frst, body)
                if log1:
                    body1 = body.replace((log1.group(0)), "")
                else:
                    body1 = body
                log2 = re.search(pattern_addition_frst, body1)
                if log2:
                    body2 = body1.replace((log2.group(0)), "")
                else:
                    body2 = body1
                log3 = re.search(pattern_fix_frst, body2)
                if log3:
                    body3 = body2.replace((log3.group(0)), "")
                else:
                    body3 = body2
                log4 = re.search(pattern_hijackthis, body3)
                if log4:
                    body4 = body3.replace((log4.group(0)), "")
                    #print(log4.group(0))
                else:
                    body4 = body3
                body4 = body4.replace(':\\','')
                body4 = body4.replace('\\','')
                word_tokens = nltk.wordpunct_tokenize(body4)
                filtered_body = []
                for w in word_tokens:
                    w = lemmatizer.lemmatize(w)
                    w = lemmatizer.lemmatize(w, 'v')
                    w = w.lower()
                    if w not in stop_words and w in words:
                        filtered_body.append(w)
                #print(filtered_body)
                body_dict["body"] = filtered_body
                body_list.append(body_dict)
                #print(body_dict)
        #print(body_list)
        return body_list

data_list = read_data()
body_list = load_body()
all_list = []
i = 0
for item in data_list:
    i += 1
    print(i)
    for body in body_list:
        if item[0] == body["thread_id"]:
            item[7] = item[7].replace('\n','')
            item.append(body["body"])
            #print("body222222",body["body"])
            #print(item)
            all_list.append(item)
print(len(body_list))
#print(all_list)

with open("data_all_body.csv", "w", encoding="utf8") as f8:
    writer = csv.writer(f8)
    writer.writerows(all_list)